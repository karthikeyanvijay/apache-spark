default:
  spark.app.name: "sparklyr-ACID-config"
  spark.sql.htl.sparkCapabilities: "CONNECTORREAD,HIVEFULLACIDREAD,HIVEFULLACIDWRITE,HIVEMANAGESTATS,HIVECACHEINVALIDATE,CONNECTORWRITE,SPARKSQL,EXTREAD,EXTWRITE,HIVESQL,HIVEBUCKET2"
  spark.sql.extensions: "com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension"
  spark.hadoop.hive.metastore.uris: "thrift://metastore-host.cloudera.site:9083"
  spark.kryo.registrator: "com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator"
  #spark.sql.extensions: "com.hortonworks.spark.sql.rule.Extensions"
  spark.datasource.hive.warehouse.read.mode: "DIRECT_READER_V1"
  spark.sql.hive.hiveserver2.jdbc.url: "jdbc:hive2://hiveserer2-host.cloudera.site:10000/datalake"
  spark.sql.hive.hiveserver2.jdbc.url.principal: "hive/hiveserer2-host@KERBEROSDOMAIN.CLOUDERA.SITE"
  sparklyr.jars.default: "/opt/cloudera/parcels/CDH/jars/hive-warehouse-connector-assembly-1.0.0.7.2.7.2-1.jar"
  spark.r.command: "./r_env/bin/Rscript"
  sparklyr.apply.env.R_HOME: "./r_env/lib/R"
  sparklyr.apply.env.RHOME: "./r_env"
  sparklyr.apply.env.R_SHARE_DIR: "./r_env/lib/R/share"
  sparklyr.apply.env.R_INCLUDE_DIR: "./r_env/lib/R/include"
  spark.driver.memory: "24G"
  spark.yarn.driver.memoryOverhead: "4G"
  spark.executor.memory: "2G"
  spark.yarn.executor.memoryOverhead: "28G"
  spark.executor.instances: "150"
  spark.executor.cores: "4"
  spark.network.timeout: "10000001"
  spark.executor.heartbeatInterval: "10000000"
  spark.yarn.queue: "default"
  spark.yarn.dist.archives: "/hadoopfs/fs1/code/r_env.tar.gz#r_env"
  spark.yarn.dist.files:  "scoring.Rdata#scoring.Rdata"
